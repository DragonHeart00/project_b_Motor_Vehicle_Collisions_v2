{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the Motor Vehicle Collisions dataset and its relevance to traffic safety in New York City."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "import datetime as dt\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"Motor_Vehicle_Collisions.csv\")\n",
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert 'CRASH DATE' to datetime format\n",
    "data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])\n",
    "\n",
    "# Extract hour, day, month, and year from the 'CRASH DATE' column\n",
    "data['HOUR'] = data['CRASH DATE'].dt.hour\n",
    "data['DAY'] = data['CRASH DATE'].dt.day\n",
    "data['MONTH'] = data['CRASH DATE'].dt.month\n",
    "data['YEAR'] = data['CRASH DATE'].dt.year\n",
    "\n",
    "# Fill missing values in the 'BOROUGH' column with 'UNKNOWN'\n",
    "data['BOROUGH'] = data['BOROUGH'].fillna('UNKNOWN')\n",
    "\n",
    "# Fill missing values in the 'ZIP CODE' column with the mode (most frequent value)\n",
    "data['ZIP CODE'] = data['ZIP CODE'].fillna(data['ZIP CODE'].mode().iloc[0])\n",
    "\n",
    "# Fill missing values in 'LATITUDE' and 'LONGITUDE' columns with the mean of each column\n",
    "data['LATITUDE'] = data['LATITUDE'].fillna(data['LATITUDE'].mean())\n",
    "data['LONGITUDE'] = data['LONGITUDE'].fillna(data['LONGITUDE'].mean())\n",
    "\n",
    "# Fill missing values in contributing factor columns with 'UNSPECIFIED'\n",
    "contributing_factor_columns = ['CONTRIBUTING FACTOR VEHICLE 1', 'CONTRIBUTING FACTOR VEHICLE 2', 'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5']\n",
    "for col in contributing_factor_columns:\n",
    "    data[col] = data[col].fillna('UNSPECIFIED')\n",
    "\n",
    "# Fill missing values in vehicle type columns with 'UNKNOWN'\n",
    "vehicle_type_columns = ['VEHICLE TYPE CODE 1', 'VEHICLE TYPE CODE 2', 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5']\n",
    "for col in vehicle_type_columns:\n",
    "    data[col] = data[col].fillna('UNKNOWN')\n",
    "\n",
    "# Verify if there are any missing values left\n",
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your dataset is stored in a DataFrame called 'data'\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'BOROUGH' column with 'Unknown'\n",
    "data['BOROUGH'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Fill missing values in the 'ZIP CODE' column with the most frequent value\n",
    "most_frequent_zip = data['ZIP CODE'].mode().iloc[0]\n",
    "data['ZIP CODE'].fillna(most_frequent_zip, inplace=True)\n",
    "\n",
    "# Drop the 'OFF STREET NAME' column if it has a large number of missing values and isn't crucial for our analysis\n",
    "data.drop('OFF STREET NAME', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_after = data.isnull().sum()\n",
    "print(missing_values_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              ---- Let's Start the data cleaning----\n",
    "We need to make sure the data is clean before starting our analysis. As a reminder, we should check for:\n",
    "\n",
    "1-Duplicate records\n",
    "2-Consistent formatting\n",
    "3-Missing values\n",
    "4-Obviously wrong values (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_rows = data.duplicated().sum()\n",
    "dup_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will drop duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the missing values in 'LOCATION' column\n",
    "data = data[data['LOCATION'].notna()]\n",
    "data = data[data['CONTRIBUTING FACTOR VEHICLE 1'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                         visualize the distribution of all numerical variables in our dataset using histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Select only the numerical columns in the dataset\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Set the number of plots per row\n",
    "plots_per_row = 3\n",
    "\n",
    "# Calculate the number of rows needed for the subplots\n",
    "num_rows = (len(numerical_columns) + plots_per_row - 1) // plots_per_row\n",
    "\n",
    "# Create a figure and axes for the subplots\n",
    "fig, axes = plt.subplots(num_rows, plots_per_row, figsize=(15, num_rows * 5))\n",
    "\n",
    "# Loop through the numerical columns and create a histogram for each one\n",
    "for i, col_name in enumerate(numerical_columns):\n",
    "    row = i // plots_per_row\n",
    "    col = i % plots_per_row\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    data[col_name].hist(bins=30, ax=ax)\n",
    "    ax.set_xlabel(col_name)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {col_name}')\n",
    "\n",
    "# Adjust the layout of the subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the histograms\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Select only the numerical columns in the dataset\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Create a scatterplot matrix using Seaborn's pairplot function\n",
    "sns.pairplot(data[numerical_columns], corner=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Select only the numerical columns in the dataset\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = data[numerical_columns].corr()\n",
    "\n",
    "# Display the correlation matrix as a table\n",
    "display(correlation_matrix.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Let's analyze some of the relationships:\n",
    "\n",
    "LATITUDE and LONGITUDE: The correlation coefficient is -0.96, indicating a strong negative correlation. As the latitude increases, the longitude decreases, and vice versa.\n",
    "\n",
    "NUMBER OF PERSONS INJURED and NUMBER OF MOTORIST INJURED: The correlation coefficient is 0.91, indicating a strong positive correlation. This means that when the number of motorists injured increases, the number of persons injured also tends to increase.\n",
    "\n",
    "NUMBER OF PERSONS KILLED and NUMBER OF PEDESTRIANS KILLED: The correlation coefficient is 0.70, indicating a moderate to strong positive correlation. This suggests that when the number of pedestrians killed increases, the number of persons killed also tends to increase.\n",
    "\n",
    "NUMBER OF CYCLIST INJURED and NUMBER OF PEDESTRIANS INJURED: The correlation coefficient is -0.03, indicating a very weak negative correlation. This suggests that there is almost no relationship between the number of cyclists injured and the number of pedestrians injured.\n",
    "\n",
    "NUMBER OF PERSONS INJURED and COLLISION_ID: The correlation coefficient is 0.05, indicating a very weak positive correlation. This suggests that there is almost no relationship between the number of persons injured and the collision ID.\n",
    "\n",
    "It's important to note that correlation does not imply causation. Just because two variables are correlated does not mean that one variable causes the other. It's also possible that an external factor influences both variables, or the correlation may simply be a coincidence. Always consider the context and domain knowledge when interpreting correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Select only the numerical columns in the dataset\n",
    "numerical_columns = data.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate the correlation matrix using pandas\n",
    "correlation_matrix = data[numerical_columns].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a correlation matrix showing the correlation coefficients between pairs of numerical variables in your dataset. You can interpret the relationships between the variables based on these coefficients. A positive value indicates a positive relationship, a negative value indicates a negative relationship, and a value close to 0 indicates no relationship. The closer the value is to -1 or 1, the stronger the relationship.\n",
    "\n",
    "Here are some insights from the correlation matrix:\n",
    "\n",
    "LATITUDE and LONGITUDE have a strong negative correlation (-0.962222). This suggests that as latitude increases, longitude decreases, and vice versa.\n",
    "\n",
    "There's a strong positive relationship between NUMBER OF PERSONS INJURED and NUMBER OF MOTORIST INJURED (0.907074). This indicates that when more people are injured in a collision, it is likely that more motorists are injured as well.\n",
    "\n",
    "NUMBER OF PERSONS KILLED has a strong positive relationship with NUMBER OF PEDESTRIANS KILLED (0.698555), NUMBER OF CYCLIST KILLED (0.278279), and NUMBER OF MOTORIST KILLED (0.656525). This suggests that when more people are killed in a collision, it is likely that more pedestrians, cyclists, or motorists are killed as well.\n",
    "\n",
    "Other relationships between variables are weak or negligible, as the correlation coefficients are close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                              ----Done with data cleaning----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    -----------     Summary statistics      ---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate basic statistics for the dataset\n",
    "basic_stats = data.describe()\n",
    "\n",
    "# Print the basic statistics\n",
    "print(basic_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are nearly 2 million rows in the dataset.The latitude values range from 0 to 43.34444, while the longitude values range from -201.36 to 0. These ranges suggest that there may be some errors or inconsistencies in the data. The average number of persons injured in each collision is 0.297, while the average number of persons killed is 0.0014. This suggests that most collisions are relatively minor and result in few or no injuries or fatalities. Similarly, the average number of pedestrians and cyclists injured or killed in each collision is also quite low.\n",
    "The average number of motorists injured in each collision is 0.216, which is higher than the average number of pedestrians or cyclists injured, but still relatively low.\n",
    "The average number of collisions per collision ID is 3.09, which suggests that there are many repeat incidents in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data has been loaded into a pandas DataFrame called 'data'\n",
    "\n",
    "# Group data by year and count the number of collisions\n",
    "collisions_by_year = data['YEAR'].value_counts().sort_index()\n",
    "print(\"Collisions by year:\")\n",
    "print(collisions_by_year)\n",
    "\n",
    "# Group data by month and count the number of collisions\n",
    "collisions_by_month = data['MONTH'].value_counts().sort_index()\n",
    "print(\"\\nCollisions by month:\")\n",
    "print(collisions_by_month)\n",
    "\n",
    "# Group data by day and count the number of collisions\n",
    "collisions_by_day = data['DAY'].value_counts().sort_index()\n",
    "print(\"\\nCollisions by day:\")\n",
    "print(collisions_by_day)\n",
    "\n",
    "# Group data by hour and count the number of collisions\n",
    "collisions_by_hour = data['HOUR'].value_counts().sort_index()\n",
    "print(\"\\nCollisions by hour:\")\n",
    "print(collisions_by_hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see  number of collisions per year increased steadily from 2012 to 2018, with a peak of 230871 collisions in 2018. After 2018, the number of collisions decreased significantly, with only 26841 collisions reported in 2023.When looking at the data by month, it appears that the number of collisions is relatively consistent throughout the year, with a slight increase in the summer months (June to August) and a peak in October. The data by day shows that the number of collisions is relatively consistent throughout the month, with no significant differences between days. However, it is worth noting that the number of collisions on the 31st is significantly lower than on other days. This is likely due to the fact that not all months have 31 days.\n",
    "t is possible that the COVID-19 pandemic had an impact on the number of collisions in 2019, 2020, and 2022. During the pandemic, many countries implemented lockdowns and restrictions on movement to slow the spread of the virus. This likely led to a decrease in the number of vehicles on the road and a corresponding decrease in the number of collisions. In addition, many people may have changed their transportation patterns during the pandemic, opting to work from home or use alternative modes of transportation such as walking or biking. This could also have contributed to a decrease in the number of collisions. It is worth noting that the data you provided shows a significant decrease in the number of collisions in 2020 and 2022 compared to previous years. This is consistent with the hypothesis that the COVID-19 pandemic had an impact on the number of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 most common contributing factors\n",
    "most_common_factors = data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().head(10)\n",
    "print(\"Top 10 most common contributing factors:\")\n",
    "print(most_common_factors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common contributing factor to collisions is \"Unspecified,\" followed by \"Driver Inattention/Distraction\" and \"Failure to Yield Right-of-Way.\" These three factors alone account for a significant proportion of collisions. Other common contributing factors include \"Following Too Closely,\" \"Backing Unsafely,\" and \"Other Vehicular.\" These factors suggest that driver behavior plays a significant role in many collisions. It is worth noting that \"Fatigued/Drowsy\" is also among the top 10 most common contributing factors. This suggests that driver fatigue may be an important issue to address in efforts to reduce the number of collisions. Overall, this data provides some useful insights into the factors that contribute to collisions. Understanding these factors can help inform efforts to improve road safety and reduce the number of collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the top 10 most common vehicle types involved in collisions\n",
    "most_common_vehicles = data['VEHICLE TYPE CODE 1'].value_counts().head(10)\n",
    "print(\"Top 10 most common vehicle types involved in collisions:\")\n",
    "print(most_common_vehicles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedans and station wagons/sport utility vehicles are the most common types of vehicles involved in collisions. These two vehicle types alone account for a significant proportion of collisions.\n",
    "It is also worth noting that passenger vehicles and sport utility/station wagon vehicles are also among the top 10 most common vehicle types involved in collisions. This suggests that these types of vehicles are commonly involved in collisions. Other common vehicle types involved in collisions include taxis, pick-up trucks, and vans. The presence of taxis on this list may be due to the fact that taxis are often on the road for longer periods of time and may be more likely to be involved in collisions as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by borough and count the number of collisions\n",
    "collisions_by_borough = data['BOROUGH'].value_counts()\n",
    "print(\"Collisions by borough:\")\n",
    "print(collisions_by_borough)\n",
    "\n",
    "# Group data by zip code and count the number of collisions\n",
    "collisions_by_zip_code = data['ZIP CODE'].value_counts()\n",
    "print(\"\\nCollisions by zip code:\")\n",
    "print(collisions_by_zip_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The borough with the highest number of collisions is Brooklyn, followed by Queens and Manhattan. The borough with the lowest number of collisions is Staten Island."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude records with \"UNKNOWN\" vehicle type\n",
    "filtered_vehicle_data = data[data['VEHICLE TYPE CODE 1'] != 'UNKNOWN']\n",
    "\n",
    "# Find the most common vehicle types involved in collisions, excluding \"UNKNOWN\"\n",
    "most_common_vehicles = filtered_vehicle_data['VEHICLE TYPE CODE 1'].value_counts().head(10)\n",
    "\n",
    "print(\"Most common vehicle types involved in collisions, excluding 'UNKNOWN':\")\n",
    "print(most_common_vehicles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is the DataFrame containing the cleaned dataset\n",
    "contributing_factors = data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()\n",
    "\n",
    "# Display the top 10 most common contributing factors\n",
    "print(contributing_factors.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do the number of collisions vary by time (hour, day, month, year)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is the DataFrame containing the cleaned dataset\n",
    "data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])\n",
    "data['CRASH TIME'] = pd.to_datetime(data['CRASH TIME'])\n",
    "\n",
    "# Extract hour, day, month, and year\n",
    "data['HOUR'] = data['CRASH TIME'].dt.hour\n",
    "data['DAY'] = data['CRASH DATE'].dt.day\n",
    "data['MONTH'] = data['CRASH DATE'].dt.month\n",
    "data['YEAR'] = data['CRASH DATE'].dt.year\n",
    "\n",
    "# Group data by year, month, day, and hour, and count the number of collisions\n",
    "collisions_by_year = data['YEAR'].value_counts().sort_index()\n",
    "collisions_by_month = data['MONTH'].value_counts().sort_index()\n",
    "collisions_by_day = data['DAY'].value_counts().sort_index()\n",
    "collisions_by_hour = data['HOUR'].value_counts().sort_index()\n",
    "\n",
    "# Display the number of collisions by time\n",
    "print(\"Collisions by year:\")\n",
    "print(collisions_by_year)\n",
    "print(\"\\nCollisions by month:\")\n",
    "print(collisions_by_month)\n",
    "print(\"\\nCollisions by day:\")\n",
    "print(collisions_by_day)\n",
    "print(\"\\nCollisions by hour:\")\n",
    "print(collisions_by_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'data' is the DataFrame containing the cleaned dataset\n",
    "\n",
    "# Group data by boroughs and count the number of collisions\n",
    "collisions_by_borough = data['BOROUGH'].value_counts()\n",
    "\n",
    "# Group data by zip codes and count the number of collisions\n",
    "collisions_by_zip = data['ZIP CODE'].value_counts()\n",
    "\n",
    "# Display the number of collisions by borough and zip code\n",
    "print(\"Collisions by borough:\")\n",
    "print(collisions_by_borough)\n",
    "print(\"\\nCollisions by zip code:\")\n",
    "print(collisions_by_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming 'data' is the DataFrame containing the cleaned dataset\n",
    "\n",
    "# Combine all 'VEHICLE TYPE CODE' columns into a single Series\n",
    "vehicle_types = pd.concat([data['VEHICLE TYPE CODE 1'], data['VEHICLE TYPE CODE 2'],\n",
    "                           data['VEHICLE TYPE CODE 3'], data['VEHICLE TYPE CODE 4'],\n",
    "                           data['VEHICLE TYPE CODE 5']])\n",
    "\n",
    "# Count the occurrences of each vehicle type\n",
    "vehicle_type_counts = vehicle_types.value_counts()\n",
    "\n",
    "# Display the most common vehicle types involved in collisions\n",
    "print(\"Most common vehicle types involved in collisions:\")\n",
    "print(vehicle_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming our data is stored in a DataFrame called \"data\"\n",
    "numerical_columns = [\n",
    "    'LATITUDE',\n",
    "    'LONGITUDE',\n",
    "    'NUMBER OF PERSONS INJURED',\n",
    "    'NUMBER OF PERSONS KILLED',\n",
    "    'NUMBER OF PEDESTRIANS INJURED',\n",
    "    'NUMBER OF PEDESTRIANS KILLED',\n",
    "    'NUMBER OF CYCLIST INJURED',\n",
    "    'NUMBER OF CYCLIST KILLED',\n",
    "    'NUMBER OF MOTORIST INJURED',\n",
    "    'NUMBER OF MOTORIST KILLED',\n",
    "]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data[numerical_columns])\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=numerical_columns)\n",
    "\n",
    "# Replace the original columns with the standardized ones\n",
    "data[numerical_columns] = standardized_df[numerical_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Standardize or normalize data if required, especially when working with different scales or units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the numerical columns\n",
    "numerical_columns = ['LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
    "                     'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
    "                     'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "                     'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED']\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary_stats = data[numerical_columns].describe()\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a bar chart of collisions by borough\n",
    "borough_counts = data['BOROUGH'].value_counts()\n",
    "plt.bar(borough_counts.index, borough_counts.values)\n",
    "plt.title('Collisions by Borough')\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5)) # Set figure size\n",
    "plt.hist(data['HOUR'], bins=24)\n",
    "plt.title('Collisions by Hour')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.xticks(range(0, 24)) # Set x-axis ticks from 0 to 23\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a horizontal bar chart of vehicle types\n",
    "vehicle_counts = data['VEHICLE TYPE CODE 1'].value_counts()\n",
    "plt.barh(vehicle_counts.index[:10], vehicle_counts.values[:10])\n",
    "plt.title('Vehicle Types Involved in Collisions')\n",
    "plt.xlabel('Number of Collisions')\n",
    "plt.ylabel('Vehicle Type')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_counts = data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.bar(factor_counts.index, factor_counts.values)\n",
    "ax.set_title('Contributing Factors')\n",
    "ax.set_xlabel('Factor')\n",
    "ax.set_ylabel('Number of Collisions')\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform exploratory data analysis (EDA):\n",
    "a. Calculate basic summary statistics, such as mean, median, mode, standard deviation, etc., to get an overall understanding of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: The mean or average of a numerical column can be calculated using the mean() function in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean, median, mode, and standard deviation for numerical columns\n",
    "for col in numerical_columns:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Mean: {data[col].mean()}\")\n",
    "    print(f\"Median: {data[col].median()}\")\n",
    "    print(f\"Mode: {data[col].mode()[0]}\")\n",
    "    print(f\"Standard Deviation: {data[col].std()}\")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', \n",
    "                     'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED', \n",
    "                     'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED', \n",
    "                     'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED']\n",
    "\n",
    "for column in numerical_columns:\n",
    "    print(f\"Summary Statistics for {column}\")\n",
    "    print(f\"Mean: {data[column].mean()}\")\n",
    "    print(f\"Median: {data[column].median()}\")\n",
    "    print(f\"Mode: {data[column].mode()[0]}\")\n",
    "    print(f\"Standard Deviation: {data[column].std()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by borough and calculate the mean number of persons injured and killed\n",
    "borough_summary = data.groupby('BOROUGH')[['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED']].mean()\n",
    "\n",
    "# Print the summary statistics for each borough\n",
    "print(borough_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a scatter plot of persons injured vs persons killed\n",
    "plt.scatter(data['NUMBER OF PERSONS INJURED'], data['NUMBER OF PERSONS KILLED'])\n",
    "plt.title('Persons Injured vs Persons Killed in Motor Vehicle Collisions')\n",
    "plt.xlabel('Number of Persons Injured')\n",
    "plt.ylabel('Number of Persons Killed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data['NUMBER OF PERSONS INJURED'].corr(data['NUMBER OF PERSONS KILLED'])\n",
    "print(f\"Correlation between number of persons injured and killed: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems that there is a very weak positive correlation between the number of persons injured and the number of persons killed in the motor vehicle collisions dataset. However, it's important to note that correlation does not imply causation and there may be other factors at play that influence the relationship between these variables. Additionally, it would be useful to visualize this relationship using a scatter plot to gain a better understanding of the nature and strength of the correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CRASH DATE to datetime format\n",
    "data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'], format='%m/%d/%Y')\n",
    "\n",
    "# Create a new column for the date only\n",
    "data['DATE'] = data['CRASH DATE'].dt.date\n",
    "\n",
    "# Group the data by date and count the number of collisions\n",
    "collisions_per_day = data.groupby('DATE').size().reset_index(name='counts')\n",
    "\n",
    "# Create the time series plot\n",
    "plt.plot(collisions_per_day['DATE'], collisions_per_day['counts'])\n",
    "plt.title('Number of Collisions by Day')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Collisions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select the relevant numerical columns\n",
    "num_cols = ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
    "            'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED',\n",
    "            'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "            'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED']\n",
    "\n",
    "# Create box plots for each numerical column\n",
    "for col in num_cols:\n",
    "    sns.boxplot(x=data[col])\n",
    "    plt.title(col)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select the numerical columns for clustering\n",
    "num_cols = ['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED', 'NUMBER OF PEDESTRIANS INJURED',\n",
    "            'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED', 'NUMBER OF CYCLIST KILLED',\n",
    "            'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED']\n",
    "\n",
    "# Standardize the data\n",
    "data_std = (data[num_cols] - data[num_cols].mean()) / data[num_cols].std()\n",
    "\n",
    "# Apply PCA to reduce the dimensions\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data_std)\n",
    "\n",
    "# Apply KMeans clustering to the reduced data\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(data_pca)\n",
    "labels = kmeans.predict(data_pca)\n",
    "\n",
    "# Visualize the clusters using a scatter plot\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('KMeans Clustering with PCA')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only columns with numerical data\n",
    "numerical_cols = data.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr_matrix = data[numerical_cols].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix shows the correlation coefficients between all pairs of numeric variables in the dataset. The values in the matrix range from -1 to 1, where 1 indicates a perfect positive correlation, 0 indicates no correlation, and -1 indicates a perfect negative correlation.\n",
    "\n",
    "From the matrix, we can see that latitude and longitude are strongly negatively correlated, which is expected since they represent the same information in opposite directions. We can also see that the number of persons injured and the number of motorist injured are strongly positively correlated, which makes sense since motor vehicles are often involved in accidents that cause injuries.\n",
    "\n",
    "Additionally, we can see that there is a moderate positive correlation between the number of persons injured and the cluster variable, which indicates that there may be some relationship between the location of accidents and the severity of injuries. However, further analysis is needed to investigate this relationship more deeply.\n",
    "\n",
    "Overall, the correlation matrix provides a useful starting point for identifying potential relationships between variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib seaborn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your dataset is in a Pandas DataFrame named 'data'\n",
    "\n",
    "# Bar chart\n",
    "top_n = 15\n",
    "vehicle_type_counts = data['VEHICLE TYPE CODE 1'].value_counts().head(top_n)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=vehicle_type_counts.index, y=vehicle_type_counts.values)\n",
    "plt.title('Number of Accidents by Vehicle Type (Top 15)', fontsize=14)\n",
    "plt.xlabel('Vehicle Type', fontsize=12)\n",
    "plt.ylabel('Number of Accidents', fontsize=12)\n",
    "plt.xticks(rotation=90, fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='LONGITUDE', y='LATITUDE', data=data, alpha=0.5, s=10, hue='BOROUGH', palette='viridis')\n",
    "plt.title('Accidents Distribution by Location', fontsize=14)\n",
    "plt.xlabel('Longitude', fontsize=12)\n",
    "plt.ylabel('Latitude', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap (correlation matrix)\n",
    "plt.figure(figsize=(12, 6))\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={\"size\": 8})\n",
    "plt.title('Correlation Matrix Heatmap', fontsize=14)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming our dataset is in a Pandas DataFrame named 'data'\n",
    "\n",
    "# Bar chart (Top 15 Vehicle Types)\n",
    "top_n = 15\n",
    "vehicle_type_counts = data['VEHICLE TYPE CODE 1'].value_counts().head(top_n)\n",
    "fig = px.bar(x=vehicle_type_counts.index, y=vehicle_type_counts.values,\n",
    "             labels={'x': 'Vehicle Type', 'y': 'Number of Accidents'},\n",
    "             title='Number of Accidents by Vehicle Type (Top 15)')\n",
    "fig.show()\n",
    "\n",
    "# Interactive scatter plot\n",
    "fig = px.scatter(data_frame=data, x='LONGITUDE', y='LATITUDE',\n",
    "                 color='BOROUGH', hover_name='COLLISION_ID',\n",
    "                 title='Accidents Distribution by Location',\n",
    "                 labels={'LONGITUDE': 'Longitude', 'LATITUDE': 'Latitude', 'BOROUGH': 'Borough'})\n",
    "fig.update_traces(marker=dict(size=6, opacity=0.7))\n",
    "fig.show()\n",
    "\n",
    "# Heatmap (correlation matrix)\n",
    "correlation_matrix = data.corr()\n",
    "fig = px.imshow(correlation_matrix, color_continuous_scale='RdBu_r', zmin=-1, zmax=1)\n",
    "fig.update_layout(title='Correlation Matrix Heatmap',\n",
    "                  xaxis=dict(tickmode='array', tickvals=list(range(len(correlation_matrix.columns))), ticktext=correlation_matrix.columns),\n",
    "                  yaxis=dict(tickmode='array', tickvals=list(range(len(correlation_matrix.index))), ticktext=correlation_matrix.index))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming your dataset is in a Pandas DataFrame named 'data'\n",
    "\n",
    "# Remove rows with missing borough or contributing factor\n",
    "filtered_data = data.dropna(subset=['BOROUGH', 'CONTRIBUTING FACTOR VEHICLE 1'])\n",
    "\n",
    "# Get the top 10 contributing factors\n",
    "top_factors = filtered_data['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Filter the data to only include the top 10 contributing factors\n",
    "filtered_data = filtered_data[filtered_data['CONTRIBUTING FACTOR VEHICLE 1'].isin(top_factors)]\n",
    "\n",
    "# Create the Sunburst chart\n",
    "fig = px.sunburst(filtered_data, path=['BOROUGH', 'CONTRIBUTING FACTOR VEHICLE 1'], title='Accidents by Borough and Top 10 Contributing Factors')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming our dataset is in a Pandas DataFrame named 'data'\n",
    "\n",
    "# Convert 'CRASH DATE' to datetime objects\n",
    "data['CRASH_DATE'] = pd.to_datetime(data['CRASH DATE'])\n",
    "\n",
    "# Set the 'CRASH_DATE' column as the index\n",
    "data.set_index('CRASH_DATE', inplace=True)\n",
    "\n",
    "# Resample the data to get the number of accidents per month\n",
    "monthly_accidents = data.resample('M').size()\n",
    "\n",
    "# Plot the number of accidents per month\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(monthly_accidents)\n",
    "plt.title('Number of Accidents per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DAY_OF_WEEK'] = data['CRASH DATE'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TOTAL_PERSONS'] = data['NUMBER OF PERSONS INJURED'] + data['NUMBER OF PERSONS KILLED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['IS_WEEKEND'] = data['DAY_OF_WEEK'].isin([5, 6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'CRASH DATE' and 'CRASH TIME' to datetime objects\n",
    "data['CRASH_DATE'] = pd.to_datetime(data['CRASH DATE'])\n",
    "data['CRASH_TIME'] = pd.to_datetime(data['CRASH TIME'], format='%H:%M').dt.time\n",
    "\n",
    "# Create a separate column for the hour of the crash\n",
    "data['CRASH_HOUR'] = data['CRASH_TIME'].apply(lambda x: x.hour)\n",
    "\n",
    "def get_time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return \"morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        return \"afternoon\"\n",
    "    elif 18 <= hour < 24:\n",
    "        return \"evening\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "data['TIME_OF_DAY'] = data['CRASH_HOUR'].apply(get_time_of_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TOTAL_PEDESTRIAN_CASUALTIES'] = data['NUMBER OF PEDESTRIANS INJURED'] + data['NUMBER OF PEDESTRIANS KILLED']\n",
    "data['TOTAL_CYCLIST_CASUALTIES'] = data['NUMBER OF CYCLIST INJURED'] + data['NUMBER OF CYCLIST KILLED']\n",
    "data['TOTAL_MOTORIST_CASUALTIES'] = data['NUMBER OF MOTORIST INJURED'] + data['NUMBER OF MOTORIST KILLED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['IS_HIT_AND_RUN'] = data['CONTRIBUTING FACTOR VEHICLE 1'].str.contains('Unspecified') | data['CONTRIBUTING FACTOR VEHICLE 2'].str.contains('Unspecified')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(date):\n",
    "    year = date.year\n",
    "    seasons = {\n",
    "        \"winter\": (pd.Timestamp(year, 12, 21), pd.Timestamp(year + 1, 3, 20)),\n",
    "        \"spring\": (pd.Timestamp(year, 3, 21), pd.Timestamp(year, 6, 20)),\n",
    "        \"summer\": (pd.Timestamp(year, 6, 21), pd.Timestamp(year, 9, 22)),\n",
    "        \"fall\": (pd.Timestamp(year, 9, 23), pd.Timestamp(year, 12, 20)),\n",
    "    }\n",
    "    for season, (start_date, end_date) in seasons.items():\n",
    "        if start_date <= date <= end_date:\n",
    "            return season\n",
    "\n",
    "data['SEASON'] = data['CRASH DATE'].apply(get_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting accidents count by time of day\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='TIME_OF_DAY', data=data, order=['morning', 'afternoon', 'evening', 'night'])\n",
    "plt.title('Accidents Count by Time of Day')\n",
    "plt.xlabel('Time of Day')\n",
    "plt.ylabel('Accident Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting accidents count by day of the week and weekend/weekday\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='DAY_OF_WEEK', data=data, hue='IS_WEEKEND')\n",
    "plt.title('Accidents Count by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Accident Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting casualties count by season\n",
    "plt.figure(figsize=(10, 6))\n",
    "season_casualties = data.groupby('SEASON')[['TOTAL_PEDESTRIAN_CASUALTIES', 'TOTAL_CYCLIST_CASUALTIES', 'TOTAL_MOTORIST_CASUALTIES']].sum()\n",
    "season_casualties.plot(kind='bar', stacked=True)\n",
    "plt.title('Casualties Count by Season')\n",
    "plt.xlabel('Season')\n",
    "plt.ylabel('Casualty Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='DAY_OF_WEEK', y='TOTAL_PERSONS', data=data, estimator=sum, ci=None)\n",
    "plt.title('Total Persons Involved in Accidents by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Persons')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x='BOROUGH')\n",
    "plt.title('Accidents by Borough')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_by_factor = data.groupby('CONTRIBUTING FACTOR VEHICLE 1').sum()['TOTAL_PERSONS'].sort_values(ascending=False).head(10)\n",
    "\n",
    "data_by_factor.plot(kind='bar')\n",
    "plt.title('Total Persons Injured by Contributing Factor')\n",
    "plt.xlabel('Contributing Factor')\n",
    "plt.ylabel('Total Persons Injured')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x='VEHICLE TYPE CODE 1', order=data['VEHICLE TYPE CODE 1'].value_counts().iloc[:10].index)\n",
    "plt.title('Accidents by Vehicle Type')\n",
    "plt.xticks(rotation=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=data, x='SEASON', y='TOTAL_PERSONS', estimator=sum)\n",
    "plt.title('Total Persons Injured by Season')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will generate a heatmap displaying the correlations between all numeric variables in your dataset. Positive correlations are displayed in blue, while negative correlations are displayed in red. The stronger the correlation, the more intense the color.\n",
    "\n",
    "Keep in mind that correlation doesn't imply causation. High correlation between two variables can help identify relationships, but it's important to investigate further and consider other factors before making conclusions.\n",
    "\n",
    "In addition, multicollinearity can cause issues when building regression models. If you identify strong correlations between independent variables, consider using techniques like dimensionality reduction or variable selection to address this issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Round the correlation matrix to 2 decimal places\n",
    "rounded_corr_matrix = corr_matrix.round(2)\n",
    "\n",
    "# Print the rounded correlation matrix\n",
    "print(rounded_corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "corr_matrix = data.corr()\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5, vmin=-1, vmax=1)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for numeric columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_columns:\n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "\n",
    "# Fill missing values for categorical columns\n",
    "categorical_columns = data.select_dtypes(include=[object]).columns\n",
    "for col in categorical_columns:\n",
    "    data[col] = data[col].fillna(method='bfill').fillna(method='ffill')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['BOROUGH', 'TIME_OF_DAY', 'SEASON'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['CRASH_DATE', 'CRASH_TIME', 'LOCATION', 'ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('IS_HIT_AND_RUN', axis=1)\n",
    "y = data['IS_HIT_AND_RUN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.dtypes)\n",
    "print(y_train.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CRASH DATE'] = pd.to_datetime(data['CRASH DATE'])\n",
    "data['ZIP CODE'] = data['ZIP CODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Data cleaning and preparation\n",
    "df = pd.read_csv(\"Motor_Vehicle_Collisions.csv\",  low_memory=False)\n",
    "# We will remove the irrelevant columns for our model and any rows with missing values\n",
    "df = df[['BOROUGH', 'ZIP CODE', 'LATITUDE', 'LONGITUDE', 'NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED',\n",
    "         'NUMBER OF PEDESTRIANS INJURED', 'NUMBER OF PEDESTRIANS KILLED', 'NUMBER OF CYCLIST INJURED',\n",
    "         'NUMBER OF CYCLIST KILLED', 'NUMBER OF MOTORIST INJURED', 'NUMBER OF MOTORIST KILLED']]\n",
    "\n",
    "# Replace empty strings with NaN values\n",
    "df.replace('', np.nan, inplace=True)\n",
    "\n",
    "# Convert 'ZIP CODE' column to numeric, forcing non-numeric values to NaN\n",
    "df['ZIP CODE'] = pd.to_numeric(df['ZIP CODE'], errors='coerce')\n",
    "\n",
    "# Fill missing values with the mean of that column for numeric columns only\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "\n",
    "# One-hot encoding for the 'BOROUGH' column\n",
    "df = pd.get_dummies(df, columns=['BOROUGH'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop(['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED'], axis=1)\n",
    "y_injured = df['NUMBER OF PERSONS INJURED']\n",
    "y_killed = df['NUMBER OF PERSONS KILLED']\n",
    "X_train, X_test, y_injured_train, y_injured_test = train_test_split(X, y_injured, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_killed_train, y_killed_test = train_test_split(X, y_killed, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit a linear regression model for predicting the number of persons injured\n",
    "lr_injured = LinearRegression()\n",
    "lr_injured.fit(X_train, y_injured_train)\n",
    "\n",
    "# Fit a linear regression model for predicting the number of persons killed\n",
    "lr_killed = LinearRegression()\n",
    "lr_killed.fit(X_train, y_killed_train)\n",
    "\n",
    "# Evaluate the models\n",
    "injured_score = lr_injured.score(X_test, y_injured_test)\n",
    "killed_score = lr_killed.score(X_test, y_killed_test)\n",
    "\n",
    "print(f\"R^2 score for predicting the number of persons injured: {injured_score}\")\n",
    "print(f\"R^2 score for predicting the number of persons killed: {killed_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R^2 score (coefficient of determination) is a statistical measure that represents the proportion of the variance in the dependent variable (in this case, the number of persons injured or killed) that can be predicted by the independent variables (features) in the model. It ranges from 0 to 1, with 1 indicating that the model can perfectly predict the dependent variable based on the independent variables and 0 indicating that the model cannot make any predictions.\n",
    "\n",
    "In our case, you have the following R^2 scores:\n",
    "\n",
    "R^2 score for predicting the number of persons injured: 0.9912693647103508\n",
    "R^2 score for predicting the number of persons killed: 0.968012761315077\n",
    "These scores are very close to 1, which indicates that the linear regression models are doing an excellent job of predicting the number of persons injured and killed based on the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess our dataset as before...\n",
    "\n",
    "# Split the data into features and targets\n",
    "X = df.drop(['NUMBER OF PERSONS INJURED', 'NUMBER OF PERSONS KILLED'], axis=1)\n",
    "y_injured = df['NUMBER OF PERSONS INJURED']\n",
    "y_killed = df['NUMBER OF PERSONS KILLED']\n",
    "\n",
    "# Initialize Linear Regression models for predicting the number of persons injured and killed\n",
    "lr_injured = LinearRegression()\n",
    "lr_killed = LinearRegression()\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_injured_scores = cross_val_score(lr_injured, X, y_injured, cv=5)\n",
    "cv_killed_scores = cross_val_score(lr_killed, X, y_killed, cv=5)\n",
    "\n",
    "# Calculate the average R^2 score across the 5 folds\n",
    "injured_mean_score = np.mean(cv_injured_scores)\n",
    "killed_mean_score = np.mean(cv_killed_scores)\n",
    "\n",
    "print(f\"Mean R^2 score for predicting the number of persons injured (5-fold cross-validation): {injured_mean_score}\")\n",
    "print(f\"Mean R^2 score for predicting the number of persons killed (5-fold cross-validation): {killed_mean_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate that our models are performing well on predicting the number of persons injured and killed in motor vehicle collisions. The mean R^2 scores from 5-fold cross-validation are close to the R^2 scores obtained from the initial train-test split:\n",
    "\n",
    "Mean R^2 score for predicting the number of persons injured (5-fold cross-validation): 0.991481399024942\n",
    "Mean R^2 score for predicting the number of persons killed (5-fold cross-validation): 0.9838041399138382\n",
    "These high R^2 scores suggest that the models are able to explain a large proportion of the variance in the target variables (number of persons injured and killed) based on the input features. It also indicates that the models are likely generalizing well to unseen data, as the cross-validation scores are consistent across different folds.\n",
    "\n",
    "We can now consider our models validated and ready for deployment or further analysis. we might want to explore feature importance, test the models on new data, or examine the residuals to ensure that the model assumptions are being met. Additionally, you could investigate other algorithms or perform hyperparameter tuning to see if you can further improve the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of the lr_injured object\n",
    "print(type(lr_injured))\n",
    "\n",
    "# Check if the object has been fitted\n",
    "if hasattr(lr_injured, 'coef_'):\n",
    "    print(\"The model has been fitted.\")\n",
    "else:\n",
    "    print(\"The model has not been fitted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear regression model for predicting the number of persons injured\n",
    "lr_injured = LinearRegression()\n",
    "lr_injured.fit(X_train, y_injured_train)\n",
    "\n",
    "# Fit a linear regression model for predicting the number of persons killed\n",
    "lr_killed = LinearRegression()\n",
    "lr_killed.fit(X_train, y_killed_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_injured = lr_injured.coef_\n",
    "importance_killed = lr_killed.coef_\n",
    "\n",
    "# Print the feature importance for each model\n",
    "for i, feature in enumerate(X.columns):\n",
    "    print(f\"{feature} importance for predicting the number of persons injured: {importance_injured[i]}\")\n",
    "    print(f\"{feature} importance for predicting the number of persons killed: {importance_killed[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These outputs represent the importance of each feature in predicting the number of persons injured and killed using the linear regression models. The importance is represented by the coefficients of the linear regression models. These coefficients indicate how much the dependent variable (number of persons injured or killed) is expected to change when the corresponding feature value increases by 1 unit, holding all other features constant.\n",
    "\n",
    "For instance, the coefficient for \"NUMBER OF PEDESTRIANS INJURED\" in the persons injured model is 0.9971412315417622, meaning that for every additional pedestrian injured, the number of persons injured is expected to increase by about 0.997, holding all other factors constant.\n",
    "\n",
    "Similarly, for the persons killed model, the coefficient for \"NUMBER OF PEDESTRIANS KILLED\" is 1.0000199679308033, indicating that for each additional pedestrian killed, the number of persons killed is expected to increase by about 1.000, holding all other factors constant."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
